{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNggkqexJMy/YgdAF1mgolF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 여러개의 독립변수로부터 선형회귀 구현"],"metadata":{"id":"p2ukd0EUKCGA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvGOW3MWJVUi"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","source":["torch.manual_seed(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrdFlr7EJ9ek","executionInfo":{"status":"ok","timestamp":1673848765732,"user_tz":-540,"elapsed":267,"user":{"displayName":"SG서기","userId":"09544633388382643408"}},"outputId":"4aae06ab-ca82-41ed-f746-ccde14558d7a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd3aed7ed10>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# 훈련 데이터 3개\n","x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n","x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n","x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"],"metadata":{"id":"YDJ7LTKBKTlx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 가중치 w 3개와 편향 b 초기화 \n","w1 = torch.zeros(1, requires_grad=True)\n","w2 = torch.zeros(1, requires_grad=True)\n","w3 = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)"],"metadata":{"id":"54IO1eMqKbBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer 설정\n","optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n","        ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nw5o_UypKezp","executionInfo":{"status":"ok","timestamp":1673848891671,"user_tz":-540,"elapsed":529,"user":{"displayName":"SG서기","userId":"09544633388382643408"}},"outputId":"ca5f2479-728b-4b49-e3b4-679dd1d614a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n","Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n","Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n","Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n","Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n","Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n","Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n","Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n","Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n","Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n","Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"]}]},{"cell_type":"markdown","source":["하지만 위의 방식은 변수의 개수가 1000개라면 x_train1 ~ x_train1000을 전부\n","선언하고, w1 ~ w1000을 전부 선언해야 한다.\n","\n","-이를 해결하기 위해 행렬 곱셈 연산(또는 벡터의 내적)을 사용합니다.\n","\n","https://wikidocs.net/54841"],"metadata":{"id":"XwiPoA0YLiow"}},{"cell_type":"markdown","source":["이번에는 행렬 연산을 고려하여 파이토치로 재구현해보겠습니다.\n","이번에는 훈련 데이터 또한 행렬로 선언해야 합니다."],"metadata":{"id":"TjgLOjFuMZex"}},{"cell_type":"code","source":["x_train  =  torch.FloatTensor([[73,  80,  75], \n","                               [93,  88,  93], \n","                               [89,  91,  80], \n","                               [96,  98,  100],   \n","                               [73,  66,  70]])  \n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"],"metadata":{"id":"LketgigBMadS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x_train.shape)\n","print(y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1lVk_FdLxuD","executionInfo":{"status":"ok","timestamp":1673849355308,"user_tz":-540,"elapsed":6,"user":{"displayName":"SG서기","userId":"09544633388382643408"}},"outputId":"97fc7b73-8996-4b2f-92f3-980471d07ae1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 3])\n","torch.Size([5, 1])\n"]}]},{"cell_type":"code","source":["# 가중치와 편향 선언\n","W = torch.zeros((3, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)"],"metadata":{"id":"PVXvZig1NHmM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["여기서 주목할 점은 가중치 W의 크기가 (3 × 1) 벡터라는 점입니다. 행렬의 곱셈이 성립되려면 곱셈의 좌측에 있는 행렬의 열의 크기와 우측에 있는 행렬의 행의 크기가 일치해야 합니다. 현재 X_train의 행렬의 크기는 (5 × 3)이며, W벡터의 크기는 (3 × 1)이므로 두 행렬과 벡터는 행렬곱이 가능합니다. 행렬곱으로 가설을 선언하면 아래와 같습니다."],"metadata":{"id":"lwWuq_VJNYx1"}},{"cell_type":"code","source":["hypothesis = x_train.matmul(W) + b"],"metadata":{"id":"I26lyHq5NcF1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["가설을 행렬곱으로 간단히 정의하였습니다. 이는 앞서 x_train과 w의 곱셈이 이루어지는 각 항을 전부 기재하여 가설을 선언했던 것과 대비됩니다. 이 경우, 사용자가 독립 변수 X의 수를 후에 추가적으로 늘리거나 줄이더라도 위의 가설 선언 코드를 수정할 필요가 없습니다. 이제 해야할 일은 비용 함수와 옵티마이저를 정의하고, 정해진 에포크만큼 훈련을 진행하는 일입니다. 이를 반영한 전체 코드는 다음과 같습니다."],"metadata":{"id":"omXdLinkOpzq"}},{"cell_type":"code","source":["x_train  =  torch.FloatTensor([[73,  80,  75], \n","                               [93,  88,  93], \n","                               [89,  91,  80], \n","                               [96,  98,  100],   \n","                               [73,  66,  70]])  \n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n","\n","# 모델 초기화\n","W = torch.zeros((3, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1e-5)\n","\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n","    hypothesis = x_train.matmul(W) + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n","    ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPoZ9YIzOqtv","executionInfo":{"status":"ok","timestamp":1673850024158,"user_tz":-540,"elapsed":458,"user":{"displayName":"SG서기","userId":"09544633388382643408"}},"outputId":"39dec0f3-666b-40e6-f87e-bcc029ed16a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n","Epoch    1/20 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n","Epoch    2/20 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7862,  95.8280]) Cost: 3069.590088\n","Epoch    3/20 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n","Epoch    4/20 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n","Epoch    5/20 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n","Epoch    6/20 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n","Epoch    7/20 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n","Epoch    8/20 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n","Epoch    9/20 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n","Epoch   10/20 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n","Epoch   11/20 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n","Epoch   12/20 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n","Epoch   13/20 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n","Epoch   14/20 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n","Epoch   15/20 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n","Epoch   16/20 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n","Epoch   17/20 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n","Epoch   18/20 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n","Epoch   19/20 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n","Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n"]}]}]}